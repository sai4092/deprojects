{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1222c1a9-c6ee-4dfa-afe0-29c6d91b6b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1810a144-2c41-4b27-b99e-c0f365403543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample DataFrames\n",
    "data1 = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)]\n",
    "data2 = [(\"Alice\", \"HR\",1), (\"Bob\", \"IT\", 2), (\"David\", \"Finance\", 3)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"name\", \"id\"])\n",
    "df2 = spark.createDataFrame(data2, [\"name\", \"dept\", \"id\"])\n",
    "\n",
    "display(df1)\n",
    "display(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056b3e31-5942-454a-98e4-4d2cda9db0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Join\n",
    "joined_df = df1.join(df2, on=\"name\", how=\"inner\")\n",
    "display(joined_df)\n",
    "\n",
    "#joined_df.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b4fb948-2675-4af0-8b9b-cb3011e6243c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# groupBy\n",
    "grouped_df = df1.groupBy(\"name\").agg(sum(\"id\"))\n",
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e43f74d-8931-472a-8f1d-fd90a745b1b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# sort\n",
    "sorted_df = df1.sort(\"id\", ascending=True)\n",
    "display(sorted_df)\n",
    "\n",
    "#select * from table order by id asc, name desc\n",
    "\n",
    "sorted_df1 = df1.orderBy(desc(\"id\"))\n",
    "display(sorted_df1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cafaac4-a0d2-4319-8019-fc36505b7395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# repartition\n",
    "repartitioned_df = df1.repartition(2, \"name\")\n",
    "display(repartitioned_df)\n",
    "\n",
    "#print(df1.rdd.getNumPartitions())\n",
    "#print(repartitioned_df.rdd.getNumPartitions())\n",
    "\n",
    "# coalesce\n",
    "coalesced_df = df1.coalesce(1)\n",
    "display(coalesced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5fcb28-eb81-485f-bddc-3d43090e57b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file = '/Volumes/workspace/default/tmp/country_lookup.csv'\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(file)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e1a070b-03d7-4ac5-86db-3ff4bbc31dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "repartitioned_df = df.repartition(20)\n",
    "display(repartitioned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69e58a8-48fe-4662-9c5a-f4a175ac9110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id, sum as spark_sum\n",
    "\n",
    "# Add partition ID column\n",
    "df_with_pid = repartitioned_df.withColumn(\"partition_id\", spark_partition_id())\n",
    "\n",
    "# Count partitions\n",
    "partition_count = df_with_pid.select(\"partition_id\").distinct().count()\n",
    "\n",
    "# Calculate size (row count) per partition\n",
    "partition_sizes = df_with_pid.groupBy(\"partition_id\").count().orderBy(\"partition_id\")\n",
    "\n",
    "print(f\"Partition count: {partition_count}\")\n",
    "display(partition_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8fb1ce5-9d10-4d86-9a50-889bf340f057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# left Semi join: select rows from df1 with match in df2 on 'id'\n",
    "semi_left_join_df = df1.join(df2, on=\"id\", how=\"leftsemi\")\n",
    "display(semi_left_join_df)\n",
    "\n",
    "# Anti left join: select rows from df1 with no match in df2 on 'id'\n",
    "anti_left_join_df = df1.join(df2, on=\"id\", how=\"leftanti\")\n",
    "display(anti_left_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9644a428-aeb1-45b7-b98e-28099bd86abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"emp\")\n",
    "df2.createOrReplaceTempView(\"dept\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "SELECT e.name, e.id, d.dept\n",
    "FROM emp e\n",
    "LEFT JOIN dept d ON e.name = d.name\n",
    "ORDER BY e.id DESC\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba72314-8ed9-4602-91e5-59617f48ad31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"dept\").orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "df_with_window = df2.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "\n",
    "display(df_with_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f957be3e-f0af-4aeb-aae3-b94181941581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_window.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"df_with_window_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c57cb2aa-c6a9-4156-81eb-b18f6e7f22eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from workspace.default.df_with_window_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e4c70b-fb39-4120-97c8-2c1191a89098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()\n",
    "spark.sql(\"use default\").show()\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a270c3-7098-4f3a-8c84-657487529071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "MERGE INTO df_with_window_delta AS target\n",
    "USING df_with_window AS source\n",
    "ON target.name = source.name AND target.id = source.id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6066954406683392,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DataFrame Wide Transfromations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
